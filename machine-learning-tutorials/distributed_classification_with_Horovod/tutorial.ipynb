{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../data/\" directory.\n",
    "import os\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "   Convert centralized training to distributed training using Horovod\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod is a framework for distribution of neural network training that supports TensorFlow, PyTorch, MXNet, Keras, and Spark. It has advantage over native tensorflow and pytorch distribution schemes because it uses ring all-reduce algorithm, which makes communication between worker nodes more efficient. Conversion from centralized to distributed training scheme becomes a matter of several additional lines of code. Same distributed training script is executed for every worker and horovod manages host-specific configurations (number of workers/GPUs on each host, data distributed to each worker, communication and training coordination, etc.).\n",
    "\n",
    "In this particular example, we use a Keras implementaion of MNIST classification training. Keras provides a high-level abstraction API for TensorFlow, which is not used directly in centralized training, but is required when we want to migrate to distributed training with Horovod.\n",
    "\n",
    "We start by including tensorflow and horovod, followed by intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import horovod.keras as hvd\n",
    "\n",
    "# Initialize Horovod\n",
    "hvd.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we must extract information about available hardware and pin GPU devices to horovod workers. This is where tensorflow comes into play. While horovod supports both tensorflow v1 and v2, we use the second version as an example here.\n",
    "\n",
    "First, get the list of all GPUs on your machine and then iterate through available devices to set the memory growth option. If memory growth is enabled for a PhysicalDevice, the runtime initialization will not allocate all memory on the device. Next, we need to make GPU devices needed for horovod workers visible. hvd.local_rank() gives us the number of workers on a host and we make exactly the same amount of GPUs visible, one of each worker in the local_rank. Local_rank for each host is given as an argument during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get the list of all GPUs on a machine\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Set memory growth option to keep GPUs from initializing all memory at once\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Set the number of visible devices to be the same as horovod needs\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting training parameters, batch size and number of classes stay the same for both centralized and distributed schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this training script will be run on multiple workers at the same time, we need to choose the number of epochs to be executed on each worker. Since the number of workers may vary and also to efficiently scale the training process, we simply divide the total number of epochs by the number of horovod workers across all hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Horovod: adjust number of epochs based on number of GPUs.\n",
    "epochs = int(math.ceil(12.0 / hvd.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the MNIST dataset and prepare it for training by setting the correct order of dimensions in the input, normalize input image pixel values to be in [0, 1] range, and convert training and testing labels to be binary class matrices (one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Normalize training data\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose our classification neural network model. Two 2D convolutional layers are followed by max pooling. The output of this feature extraction is then flattened into a vector of size 128 that is put through a fully connected layer with ReLU activation. The final layer of the network is of the size of the number of classes and is activated with the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data and model preparation are core things in the training process, no matter whether its centralized or distributed. In the centralized training, next steps would be to compile and fit your model, but in order to make the process distributed, we need to add a distributed optimizer and also some callbacks. \n",
    "\n",
    "First, we take a regular optimizer like Adadelta, which is a more robust variant of Stochastic Gradient Descent (SGD) and provide it with a learning rate/step size adjusted by the number of workers. Effective batch size in synchronous distributed training is scaled by the number of workers. An increase in learning rate compensates for the increased batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.Adadelta(0.001 * hvd.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we wrap the resulting optimizer into a horovod DistributedOptimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compile our model with the horovod distributed optimizer and add the experimental_run_tf_function=False option to make sure tensorflow backend uses the horovod distributed optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling the model, we must specify some horovod callbacks to be used during training. In particular, we want the main worker to broadcast the model (weights) to all other processes to ensure that all workers have the same starting variable, and also averge training metrics among workers after each epoch is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    \n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large dataset and a complex network model, the training process may take a long time and we often want to save some intermediate results of our training in case anything goes wrong during the long training process or to simply reprodice training results. In case of distributed training with Horovod, we only want the main worker to save model and this can be done with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint('output/checkpoint-{epoch}.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step, we attach horovod callbacks, and enable verbosity only for the main worker in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks, # Horovod callbacks\n",
    "          epochs=epochs,\n",
    "          verbose=1 if hvd.rank() == 0 else 0, # Make only the main worker verbose \n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed Training:\n",
    "With these last modifications to the model.fit function, we are ready to run this MNIST classification model training in a distributed fashion. Even though this code is written in Python, we can not simply start training with python distributed_train.py. Instead, we must use horovod to start distribute process. As an example, you can run distributed_train.py script on two local GPUs from console with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horovodrun -np 2 -H localhost:2 python distributed_train.py |& grep -v \"Read -1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "horovodrun options:\n",
    "    -np - number of processes to use\n",
    "    -H - provide host information, structure is ip_address:worker#(worker# <= GPU#)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run this example automatically as a Job, check the mlsteam.yml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centralized Training: \n",
    "Simply run the centralized training script with python in the terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python centralized_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can automatically start training with a new Job by using the mlsteam.yml file. Paste the above code line there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
